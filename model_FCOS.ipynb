{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# FCOS Model\n","\n","We will use the FCOS model from the [FCOS](./documentation/fcos.pdf) paper. This model don't use anchored bounding box.\n","\n","## Imports"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.io import read_image\n","from torchvision.transforms import (\n","    Compose,\n","    Normalize,\n","    Resize,\n","    ToPILImage,\n","    ToTensor,\n",")\n","import torchvision\n","\n","from tqdm import tqdm\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Build the Dataset Class\n","\n","We will use a coco subset to train this model.\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Custom Dataset Class\n","\n","This part is not yet working."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def map_to_class(cat) -> torch.int8:\n","    map = {\n","    \"triton\": 1,\n","    \"grenouille-crapaud\": 2,\n","    \"planche\": 3,\n","    \"feuille\": 4,\n","    \"souris\": 5,\n","    \"insecte\": 6,\n","    }\n","    return map.get(cat, -1)\n","\n","\"\"\"\n","Classe pour charger les données.\n","Les images sont dans le dossier img_dir et les labels dans un fichier csv annotations_file.\n","Les labels sont des entiers [0,1] qui représentent la présence d'une planche.\n","\"\"\"\n","\n","class CrapaudDataset(Dataset):\n","\n","    def __init__(self,\n","                 annotations_file,\n","                 img_dir,\n","                 transform=None,\n","                 target_transform=None) -> None:\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = read_image(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Coco Vision Dataset\n","\n","This is the official coco custom dataset from torchvision. I don't know the difference between CocoDetection and CocoCaptions."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import os.path\n","from typing import Any, Callable, List, Optional, Tuple\n","\n","from PIL import Image\n","\n","from torchvision.datasets import VisionDataset\n","\n","\n","class CocoDetection(VisionDataset):\n","    \"\"\"`MS Coco Detection <https://cocodataset.org/#detection-2016>`_ Dataset.\n","\n","    It requires the `COCO API to be installed <https://github.com/pdollar/coco/tree/master/PythonAPI>`_.\n","\n","    Args:\n","        root (string): Root directory where images are downloaded to.\n","        annFile (string): Path to json annotation file.\n","        transform (callable, optional): A function/transform that  takes in an PIL image\n","            and returns a transformed version. E.g, ``transforms.PILToTensor``\n","        target_transform (callable, optional): A function/transform that takes in the\n","            target and transforms it.\n","        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n","            and returns a transformed version.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        root: str,\n","        annFile: str,\n","        transform: Optional[Callable] = None,\n","        target_transform: Optional[Callable] = None,\n","        transforms: Optional[Callable] = None,\n","    ) -> None:\n","        super().__init__(root, transforms, transform, target_transform)\n","        from pycocotools.coco import COCO\n","\n","        self.coco = COCO(annFile)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    def _load_image(self, id: int) -> Image.Image:\n","        path = self.coco.loadImgs(id)[0][\"file_name\"]\n","        return Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n","\n","    def _load_target(self, id: int) -> List[Any]:\n","        return self.coco.loadAnns(self.coco.getAnnIds(id))\n","\n","    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n","        id = self.ids[index]\n","        image = self._load_image(id)\n","        target = self._load_target(id)\n","\n","        if self.transforms is not None:\n","            image, target = self.transforms(image, target)\n","\n","        return image, target\n","\n","    def __len__(self) -> int:\n","        return len(self.ids)\n","\n","\n","class CocoCaptions(CocoDetection):\n","    \"\"\"`MS Coco Captions <https://cocodataset.org/#captions-2015>`_ Dataset.\n","\n","    It requires the `COCO API to be installed <https://github.com/pdollar/coco/tree/master/PythonAPI>`_.\n","\n","    Args:\n","        root (string): Root directory where images are downloaded to.\n","        annFile (string): Path to json annotation file.\n","        transform (callable, optional): A function/transform that  takes in an PIL image\n","            and returns a transformed version. E.g, ``transforms.PILToTensor``\n","        target_transform (callable, optional): A function/transform that takes in the\n","            target and transforms it.\n","        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n","            and returns a transformed version.\n","\n","    Example:\n","\n","        .. code:: python\n","\n","            import torchvision.datasets as dset\n","            import torchvision.transforms as transforms\n","            cap = dset.CocoCaptions(root = 'dir where images are',\n","                                    annFile = 'json annotation file',\n","                                    transform=transforms.PILToTensor())\n","\n","            print('Number of samples: ', len(cap))\n","            img, target = cap[3] # load 4th sample\n","\n","            print(\"Image Size: \", img.size())\n","            print(target)\n","\n","        Output: ::\n","\n","            Number of samples: 82783\n","            Image Size: (3L, 427L, 640L)\n","            [u'A plane emitting smoke stream flying over a mountain.',\n","            u'A plane darts across a bright blue sky behind a mountain covered in snow',\n","            u'A plane leaves a contrail above the snowy mountain top.',\n","            u'A mountain that has a plane flying overheard in the distance.',\n","            u'A mountain view with a plume of smoke in the background']\n","\n","    \"\"\"\n","\n","    def _load_target(self, id: int) -> List[str]:\n","        return [ann[\"caption\"] for ann in super()._load_target(id)]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data Importation\n","\n","We instantiate the dataset and the dataloader."]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n"]}],"source":["# Pipeline de transformation des images\n","img_pipeline = Compose(\n","    [\n","        ToPILImage(),\n","        Resize((256, 256)),\n","        ToTensor(),\n","    ]\n",")\n","\n","# Chargement des données\n","ds_train = CocoDetection(\n","    root=os.path.join(\"subset\",\"coco_subset\",\"train\"),\n","    annFile=os.path.join(\"subset\",\"coco_subset\", \"train.json\"),\n","    transform=None,\n",")\n","ds_val = CocoDetection(\n","    root=os.path.join(\"subset\",\"coco_subset\",\"val\"),\n","    annFile=os.path.join(\"subset\",\"coco_subset\", \"val.json\"),\n","    transform=None,\n",")\n","\n","# Hyperparamètres\n","batch_size = 4\n","validation_split = 0.2\n","shuffle_dataset = True\n","random_seed = 42\n","hp = dict(num_epochs=3, learning_rate=0.001, momentum=0.9)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train/Test Split"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(ds_val, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["torch.utils.data.dataloader._SingleProcessDataLoaderIter"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["type(iter(train_loader))"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# function to show an image\n","def imshow(img):\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","\n","# get some random training images\n","batch = iter(train_loader)\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["torch.utils.data.dataloader._SingleProcessDataLoaderIter"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["type(batch)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["images, labels = next(batch)\n","\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))\n","\n","# print labels\n","print(\" \".join(f\"{l}\" for l in labels))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Import Json\n","\n","This is a proof of concept to import a json file.\n","\n","import json\n","path = os.path.join(\"subset\",\"coco_subset\", \"train.json\")\n","with open(path,'r') as f:\n","    data = json.loads(f.read())\n","\n","df2 = pd.json_normalize(data, record_path=['images'])\n","df2"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /home/olivier/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15f314c61acc46e4a65c57e1648bdb7e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/13.6M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","import torchvision\n","from torchvision.models import MobileNet_V2_Weights\n","from torchvision.models.detection import FCOS\n","from torchvision.models.detection.anchor_utils import AnchorGenerator\n","\n","\n","# load a pre-trained model for classification and return\n","# only the features\n","backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features\n","# FCOS needs to know the number of\n","# output channels in a backbone. For mobilenet_v2, it's 1280\n","# so we need to add it here\n","backbone.out_channels = 1280\n","        # let's make the network generate 5 x 3 anchors per spatial\n","# location, with 5 different sizes and 3 different aspect\n","# ratios. We have a Tuple[Tuple[int]] because each feature\n","# map could potentially have different sizes and\n","# aspect ratios\n","anchor_generator = AnchorGenerator(\n","    sizes=((8,), (16,), (32,), (64,), (128,)),\n","    aspect_ratios=((1.0,),)\n",")\n","# put the pieces together inside a FCOS model\n","model = FCOS(\n","    backbone,\n","    num_classes=80,\n","    anchor_generator=anchor_generator,\n",")\n","model.eval()\n","x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","predictions = model(x)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"arn","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"vscode":{"interpreter":{"hash":"6f90e391dca8f8b979c746e60f5494ca9d42a24f5a0cc8a246e63e41ca743f4f"}}},"nbformat":4,"nbformat_minor":0}
