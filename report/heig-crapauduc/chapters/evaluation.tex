\chapter{Analyses}
\label{chap:Evaluation}
Nous allons commencer par expliquer les différentes métriques utilisables lors d'une tâche d'object detection. Ensuite, nous allons expliquer comment lire un benchmark COCO. Enfin, nous allons analyser l'entraînement de nos différents modèles ainsi que le score COCO obtenu. Et pour finir, nous allons observer et analyser des images qui présentent des erreurs de prédictions.

\section{Contexte}
Dans une tâche d'object detection, nous utilisons le score IoU qui signifie Intersection over Union.
C'est un score qui compare les bounding boxes prédites par le modèle avec les bounding boxes réelles (ground-truth).
Une tâche d'object detection comprend deux sous-problèmes: la classification et la localisation.
Ainsi nous avons plusieurs métriques pour analyser la performance d'un modèle. IoU se concentre sur 
la localisation des bounding boxes prédites tandis que la métrique mAP (mean Average Precision) 
se concentre sur la classification.
\begin{figure}[bh!]
    \centering
    \scalebox{0.5}[0.5]{\includegraphics[width=\textwidth]{images/iou.png}}
    \caption{Intersection over Union}
    \label{fig:iou}
\end{figure}
Un score IoU de 1 signifie que la bounding box prédite est parfaitement superposée sur la bounding box réelle, tandis qu'un score de 0 signifie qu'il n'y a pas d'air en commun. Idéalement, on espère donc avoir un score IoU de 1 pour toutes nos bounding boxes.
% -- Joris:


\section{Lecture d'un benchmark COCO}
Un benchmark COCO peut être affiché dans la console comme présenté dans l'image \ref{fig:eval_coco_benchmark}. Ce benchmark est réalisé avec l'API COCO\footnote[1]{\url{cocodataset.org/\#detection-eval}}. Il présente deux métriques la précision ($\displaystyle{\frac{TP}{TP+FP} }$) et le rappel ($\displaystyle{ \frac{TP}{TP+FN}}$).
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{images/eval_coco_benchmark_white.png}
    \caption{Exemple de benchmark COCO dans la console}
    \label{fig:eval_coco_benchmark}
\end{figure}
La précision moyenne, (AP-Average Precision) présente dans les résultats COCO est calculée sur toutes les catégories, elle correspond traditionnellement au mean Average Precision (mAP).
Les résultats sont divisé en 4 catégories qui dépendent du score IoU ou de l'air de la bounding box prédite.
\begin{itemize}
    \item Les trois premières lignes sont l'AP en considérant différents seuil d'IoU pour sélectionner les boudings boxes à évaluer.
    \item Les trois suivantes sont l'AP en considérant des surfaces de tailles différentes pour sélectionner les boudings boxes à évaluer
    \item Les trois suivantes sont le Rappel Moyens (AR) en considérant plusieurs IoU pour sélectionner les boudings boxes à évaluer.
    \item Les trois suivantes sont l'AR en considérant des surfaces de tailles différentes pour sélectionner les boudings boxes à évaluer.
\end{itemize}
Selon la $\text{documentation}^1$, la première ligne est la plus importante. A la place de considérer les bounding boxes qui ont un IoU plus grand que le seuil, il est calculé sur cette ligne la moyenne des mAP des bounding boxes selon 10 seuils différents (de 0.5 à 0.95 par pas de 0.05).
%https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md
Il faut noter que s'il n'existe pas de bounding boxes répondant aux critères, un score de \verb|-1| est affiché. On observe donc qu'il n'existe pas de bounding box de petite taille (32x32 pixels) puisque l'AP ainsi que l'AR où l'air est petite vaut \verb|-1|.
\paragraph{}
Avec Detectron2, un benchmark COCO est fait après chaque epoch. C'est à dire lorsque le modèle à vu une fois le dataset en entier. Ce benchmark est effectué sur un fold du set d'entraînement appelé validation. Cependant, afin de véritablement tester les résutlats, nous avons aussi effectué un benchmark COCO sur un set de test d'images jamais vues par le modèle. 
Detectron2 sauvegarde les résultats dans un fichier json, ce fichier peut être lu par un widget TensorBoard afin de monitorer l'entraînement.
Les résultats peuvent aussi être visualisés de manière résumées, par défaut ce n'est pas fait durant l'entraînement uniquement lors d'un benchmark COCO complet. Le résumé s'affiche comme dans l'image \ref{fig:eval_coco_benchmark_resume} 
\begin{figure}
    \centering
    \scalebox{0.5}[0.5]{\includegraphics[width=\textwidth]{images/eval_coco_benchmark_resume.png}}
    \caption{Exemple de résumé COCO}
    \label{fig:eval_coco_benchmark_resume}
\end{figure}
On voit très facilement que les AP sont désormais calculés par classes sans distinction de IoU. 

\section{L'entraînement}\label{anal:train}
Nous avons entraîné les modèles les uns après les autres sur un set de test composé de 176 tritons, 170 crapaud-grenouilles ainsi que de 150 souris. Ce dernier animal n'était pas demandé dans la tâche, mais nous nous sommes dit que ça permettrait d'ajouter de la diversité dans le dataset.
L'évaluation a été effectuée sur un set composé de 48 tritons, 47 crapaud-grenouilles et 30 souris. Les labels ont été fournis par le professeur.
Nous n'avons pas changé les paramètres généraux de Detectron2, uniquement les paramètres relatifs aux différents modèles, les entraînements se sont déroulés sur une GPU premium dont la spécification est visible dans le listing \ref{lst:gpu_spec}
\lstset{style=Bash}
\begin{lstlisting}[caption = {Résultat de la commande nvidia-smi},label={lst:gpu_spec}]
NVIDIA-SMI:     460.32.03    
Driver Version: 460.32.03    
CUDA Version:   11.2     
GPU  Name:      A100-SXM4-40GB
\end{lstlisting}
Les versions de Pytorch, ainsi que de Python étaientt celles par défaut sur colab au moment de l'entraînement.

\paragraph{}Nous allons maintenant détailler les différents entraînements effectués.

\subsection{TensorBoard de faster RCNN }\label{anal:train_rcnn}
Les widgets tensorboard permettent durant tous les entraînements d'avoir un aperçu des différentes métriques enregistrées. Nous pouvons donc savoir quand arrêter l'entraînement pour avoir la meilleur performance selon une métrique, cela permet aussi de ne pas overfit. Nous avons pris quelques captures d'écrans affichées dans les figures \ref{fig:tensorboard_overview} et \ref{fig:tensorboard_metric}
\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/tensorboard_large.png}
        \caption{Aperçu du widget Tensorboard}
        \label{fig:tensorboard_overview}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/tensorboard_metric.png}
        \caption{Aperçu de cls\_accuracy, false\_negative et foreground\_cls\_accuracy durant l'entraînement.}
        \label{fig:tensorboard_metric}
    \end{subfigure}
    % \scalebox{0.5}[0.5]{}
    % \caption{Aperçu du widget Tensorboard}
    % \label{fig:tensorboard_overview}
    % \scalebox{0.5}[0.5]{\includegraphics[width=\textwidth]{images/tensorboard_metric.png}}
    % \caption{Aperçu des métriques enregistrées dans TensorBoard}
    \caption{Aperçu de tensorboard et des métriques de faster RCNN}
\end{figure}
Detectron2 affiche aussi une loss total, nous n'avons pas réussis à déterminer la manière dont elle est calculée, mais elle reste un bon indicateur de la performance du model et de l'over/under fitting.
\begin{figure}[hb!]
    \centering
    \scalebox{0.5}[0.5]{\includegraphics[width=\textwidth]{images/loss_faster_rcnn.png}}
    \caption{Aperçu de la loss total enregistrée dans TensorBoard}
    \label{fig:loss_faster_rcnn}
\end{figure}
Comme nous pouvons le voir dans la figure \ref{fig:loss_faster_rcnn}, nous n'avons ni d'overfit, ni d'underfit.


\subsection {TensorBoard de RetinaNet}\label{anal:train_retina}
Dans cette section nous allons observer les métriques du modèle RetinaNet. Ce modèle enregistre moins de métriques, ceci est du au fait qu'il ne possède pas une architecture aussi complexe que Faster-RCNN. Alors que Faster-RCNN présentait des métriques pour certaines parties spécifiques du réseau, les RPN (Region Proposal Network) par exemple, RetinaNet ne présente que deux métriques relatives à l'entraînement : la loss de classification ainsi que la loss des bounding boxes.
\begin{figure}[h!]
    \begin{subfigure}[h]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/tensorboard_retina_net_loss_box.png}
        \caption{Loss des bounding boxes}
        \label{fig:loss_retinanet_box}
    \end{subfigure}
    \begin{subfigure}[h]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/tensorboard_retina_net_loss_cls.png}
        \caption{Loss pour la classification}
        \label{fig:loss_retinanet_cls}
    \end{subfigure}
    \caption{Les métriques de RetinaNet durant un entraînement}
\end{figure}
Nous pouvons observer que RetinaNet doit être plus entraîné, 1000 itérations contre 500 pour Faster-RCNN. Il ne faut pas confondre itérations et epoch. Une epoch représente un passage sur l'entièreté du dataset, tandis qu'une iteration est un pas vers la recherche d'un minimum durant l'optimisation de l'erreur. Nous avons déterminé ces hyperparamètres grâce à tensorboard qui permet durant l'entraînement d'avoir un aperçu des métriques. Fait intéressant, RetinaNet est plus rapide à entraîner (environ 10 minutes de moins que Faster-RCNN) ceci est surement du à son architecture plus simple qui nécessite donc moins de calcule.

% -- Vic : 

\section{Évaluations des deux modèles sélectionnés}\label{anal:evaluation}

Après avoir testé plusieurs approches comme mentionnées dans le chapitre \ref{chap:Modeles}, nous avons retenu deux modèles dont nous allons ici comparer les scores d'évaluation :

\begin{itemize}
    \item[-] Faster R-CNN avec Detectron2 et
    \item[-] Retinanet avec Detectron2.
\end{itemize}

\subsection{Scores des modèles}

Voici donc les scores obtenus avec Faster R-CNN (figure \ref{fig:scores_fasterRcnn}) et RetinaNet (figure \ref{fig:scores_retinanet}) :

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/eval_FasterRCNN_resume_annote.png}
    \caption{Scores obtenus avec Faster R-CNN}
    \label{fig:scores_fasterRcnn}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/eval_Retinanet_resume_annote.png}
    \caption{Scores obtenus avec RetinaNet}
    \label{fig:scores_retinanet}
\end{figure}

D'après le chapitre 7.2 vu plus tôt "Lecture d'un benchmark COCO", c'est la première ligne de ces résultats qui est la plus importante, c'est-à-dire la moyenne des mAP des bounding boxes qui ont un IoU entre 50 et 95 pourcent. On va donc ici observer ces valeurs, ainsi que l'average recall recall pour le même intervalle de IoU, avec la même valeur (=100) pour maxDets - nombre maximal d'objets détectables sur une image. On peut observer ces valeurs sur les figures \ref{fig:scores_fasterRcnn} et \ref{fig:scores_retinanet}, en \textit{(1)} pour l'AP et en \textit{(2)} pour l'AR ; les voicis dans ce tableau :

\begin{center}
   \begin{tabular}{ | l || c | c |}
     \hline
     modèle & average precision & average recall \\ \hline
     Faster R-CNN & 0.347 & 0.426 \\ \hline
     Retinanet & 0.260 & 0.294 \\ \hline
   \end{tabular}
 \end{center}

On voit donc premièrement que les deux scores sont meilleurs pour Faster R-CNN que pour RetinaNet. Aussi, en observant les AP par classe (\textit{(3)} sur les figures \ref{fig:scores_fasterRcnn} et \ref{fig:scores_retinanet}), on a les résultats suivants : 

\begin{center}
   \begin{tabular}{ | l || c | c |}
     \hline
     modèle & AP grenouille-crapaud & AP triton \\ \hline
     Faster R-CNN & 39.811 & 40.475 \\ \hline
     Retinanet & 49.401 & 0.000 \\ \hline
   \end{tabular}
 \end{center}
 
On voit ici que bien que le score pour la classe crapaud-grenouille soit plus élevé à l'aide du modèle RetinaNet, le score pour la classe triton est de zéro. Rappelons que la tâche initiale de ce projet est de compter le nombre de triton/crapaud-grenouille qui utilisent les crapauduc ; ainsi, nous avons choisi l'algorithme Faster R-CNN comme modèle final pour cette tâche de classification.\newline

Il est également ici intéressant de noter que le fait que la classe triton ait une AP nulle avec RetinaNet a un impact important sur le résultat de mAP. Il serait dont important et intéressant d'investiguer plus profondément sur la raison de l'échec complet de détection de triton ; comme discuté plus tard en section \ref{subsubsec:Retinanet_tritons} de ce rapport.

\subsection{Modèle final choisi}\label{anal:final_model}

\paragraph{Entraînement}

Voici donc comment nous avons premièrement entraîné notre modèle Faster R-CNN, avec Detectron 2 : 

\lstset{language=Python}
\lstset{style=Python}
\begin{lstlisting}
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/
faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("triton_train",)
cfg.DATASETS.TEST = ()
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/
faster_rcnn_X_101_32x8d_FPN_3x.yaml")  
cfg.SOLVER.IMS_PER_BATCH = 2 
cfg.SOLVER.BASE_LR = 0.00020 
cfg.SOLVER.MAX_ITER = 500 
cfg.SOLVER.STEPS = []   
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4 
\end{lstlisting}


\paragraph{Test}

Et voici donc le modèle construit pour tester nos données d'évaluation : 

\lstset{style=Python}
\begin{lstlisting}
# recuperation des poids du modele qu'on vient d'entrainer
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")  
# determination d'un treshold pour le test
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  
# construction du predicteur
predictor = DefaultPredictor(cfg)
\end{lstlisting}

Le seuil choisi pour le test ci-dessus indique donc qu'on considérera une image comme étant prédite d'une certaine classe si le modèle est sûr à au moins 70\% de sa classification.\newline

Voici deux exemples de résultats de classification par notre modèle ainsi créé :

\begin{figure}[H]
    \centering
    \includegraphics[width=300px]{images/Eval_FasterRCNN_crapGren.png}
    \caption{Prédiction de Faster R-CNN - crapaud-grenouille}
    \label{fig:fasterRcnn_crapGren}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=300px]{images/Eval_FasterRCNN_triton.png}
    \caption{Prédiction de Faster R-CNN - triton}
    \label{fig:fasterRcnn_triton}
\end{figure}

On observe donc ici une bonne prédiction du modèle, qui est quasi certain de son classement (96\% et 97\%). Il est cependant intéressant et surtout important de noter que le modèle a été entraîné sur des images qui peuvent être très semblables à celles sur lesquelles il a été évalué ensuite. En effet, les images que nous avons prises pour l'entraînement et pour l'évaluation proviennent d'un ensemble d'images pouvant provenir de la même séquence ou du même crapauduc, ce qui peut provoquer de l'overfitting. Cette problématique sera discutée plus en profondeur dans le chapitre \ref{chap:next_steps}.

\section{Où est le problème}\label{ch:eval:sex:problem}

Nous avons donc des résultats qui approchent ceux de l'état de l'art en 2019 selon \url{https://paperswithcode.com/sota/object-detection-on-coco}. Nous avons aussi des modèles qui sont correctement entraînés puisque, nous l'avons vu dans la partie \ref{anal:train_rcnn} ainsi que \ref{anal:train_retina}, les métriques indiquent qu'ils sont ni sous-entraînés ni sur-entraînés. Il nous faut donc observer des mauvaises classifications afin de comprendre les erreurs et déterminer d'autres facteurs sur lesquels nous pouvons agir afin d'avoir de meilleurs résultats. En effet, à l'heure où nous écrivons ces mots, un modèle basé sur DETR \footnote[1]{\url{https://arxiv.org/pdf/2211.03594v1.pdf}} atteint 64.5 de mAP sur le benchmark COCO, ce qui est un score très élevé. Il existe donc des modèles SOTA bien meilleurs mais nous n'arrivons pas à les reproduire. Nous allons donc observer les erreurs de classification afin de comprendre ce qui ne va pas, mais avant nous aimerions faire remarquer qu'une explication possible est simplement la taille du modèle DETR - notre plus gros modèle - qui a 60 millions de paramètres alors que leur modèle en a 600 millions.
\subsection{Analyse des images non détectés par Faster-RCNN}
\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{images/failed_pred1_rcnn.png}
        \caption{Crapaud-grenouille caché}
        \label{fig:eval_fasterRCNN_a}
    \end{subfigure}
    \begin{subfigure}[h]{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{images/failed_pred3_rcnn.png}
        \caption{Double bounding box}
        \label{fig:eval_fasterRCNN_b}
    \end{subfigure}
    \begin{subfigure}[h]{0.7\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/failed_pred5_rcnn.png}
        \caption{Erreur de classification}
        \label{fig:eval_fasterRCNN_c}
    \end{subfigure}
    \caption{Exemple d'images mal prédites par Faster-RCNN}
    \label{fig:failed_pred_rcnn}
\end{figure}
La plupart des images qui présentent des erreurs pouvant expliquer le score de Faster-RCNN ressemble aux images de la figure \ref{fig:failed_pred_rcnn}. 
Nous avons ici trois types d'erreurs. 
\paragraph{}
La première, qui est aussi une des plus fréquente, est la non détection d'animaux (souvent sur les bords de l'image). Le manque de luminosité ou la présence de feuilles pourrait expliquer cela. Un préprocessing pourrait vérifier l'hypothèse de la luminosité. Cependant, nous ne pouvons pas vraiment faire de préprocessing pour la présence de feuille.
\paragraph{}
La seconde est aussi assez fréquente, il s'agit de la présence de multiples bounding boxes sur un même animal. Même si c'est étonnant, cette erreur ne devrait pas affecter les scores sur le benchmark. Nous avons aussi tuner le seuil de confiance (treshold) pour réduire ce genre d'erreur.
\paragraph{}
La dernière est la seule occurrence de ce type observée sur le set de test ; on peut voir qu'un bout de feuille est mal classifié avec pourtant une probabilité de 90\% - et ceci sans raisons visibles.
%  Nous pouvons nous demander si la planche à vraiment été bénéfique, en effet la plupart des annotations sont sur la planche, là où la planche permet rapidment d'apercevoir un animal. Le modèle a peut-être appris qu'il est plus probable de voir un animal donc peut-être plus habitué à voir des animaux à cet endroit.
En conclusion, Faster R-CNN est vraiment bon et les erreurs ne présentent pas de graves erreurs d'entraînement.

\subsection{Analyse des images non détectées par RetinaNet}
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{images/failed_pred3_retina.png}
        \caption{Triton jamais détecté}
        \label{fig:eval_retina_a}
    \end{subfigure}
    \begin{subfigure}[h]{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{images/failed_pred5_retina_big.png}
        \caption{Non détection du crapaud}
        \label{fig:eval_retina_b}
    \end{subfigure}
    \begin{subfigure}[h]{0.7\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/failed_pred4_retina.png}
        \caption{Animal non détecté}
        \label{fig:eval_retina_c}
    \end{subfigure}
    \caption{Exemple d'images mal prédites par RetinaNet}
    \label{fig:failed_pred_retina}
\end{figure}
Nous pouvons voir dans la figure \ref{fig:failed_pred_retina} que RetinaNet ne détecte jamais les tritons, ce qui confirme les scores du benchmark COCO vu précédement. Nous avons pensé que c'était dû à la petite taille de ces derniers, cependant nous pouvons observé que l'image \ref{fig:eval_retina_b} est un gros crapaud qui n'est pas non plus reconnu. Il faut remarquer que du set de test, il s'agit de l'unique occurrence d'une telle erreur.
On peut noter que les crapauds-grenouilles sont toujours très bien détectés par RetinaNet. L'image \ref{fig:eval_retina_c} nous montre un problème similaire à celui vu précédemment sur l'image \ref{fig:eval_fasterRCNN_a}, ce qui tend à confirmer l'hypothèse du manque de lumière sur les cotés.
\paragraph{}
 Après avoir regardé toutes les prédictions sur les images du set de test pour les deux modèles, nous avons remarqué que plusieurs images semblent provenir de la même séquence. Ainsi, lorsque la détection ne fonctionne pas, elle ne fonctionne pas plusieurs fois! Ceci implique que les erreurs sont comptabilisées à double. Cependant, nous ne pouvons pas crier victoire et dire que nos scores sont bien en dessous de leur vrai valeurs car nous observons aussi ceci sur les prédictions correctes! Ainsi les bons scores et mauvais scores à double se compensent.

 \paragraph{Conclusion} Nos deux modèles semblent avoir des difficultés avec les animaux qui passent par les cotés. Il se peut, au vu des nombreuses annotations d'entraînement placées au centre de l'image, que les modèles détectent mieux les animaux au centre de l'image de par leur entraînement. Néanmoins, un ajustement de la luminosité serait une piste de réflexion.
